{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc\n",
    "Optimus class is here for fast iteration, maybe should use as seperate module later\n",
    "\n",
    "Maybe fix retain_graph if speed is a bottleneck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe642d5e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you're in the spar-red-tem/owen directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# !pip install --upgrade transformers\n",
    "# !pip3 install torch torchvision\n",
    "\n",
    "# !pip install -r ~/GENIES/requirements.txt\n",
    "# !nvidia-smi\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856ff2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# import numpy as np\n",
    "# import datasets\n",
    "# import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cacdb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string from wikipedia about shaq\n",
    "shaq = \"Shaquille O'Neal is a 7-foot-1-inch (2.16 m) and 325-pound (147 kg) center who played for six teams over his 19-year career in the National Basketball Association (NBA) and is a four-time NBA champion. O'Neal is regarded as one of the greatest basketball players and centers of all time.\"\n",
    "# A string from wikipedia about benzene\n",
    "benzene = \"Benzene is a natural constituent of petroleum and is one of the elementary petrochemicals. Due to the cyclic continuous pi bonds between the carbon atoms, benzene is classed as an aromatic hydrocarbon. Benzene is a colorless and highly flammable liquid with a sweet smell, and is partially responsible for the aroma of gasoline.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimus (code copied here for iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Optimus_dir.code.Optimus import Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Optimus_dir.code.Optimus import Optimus\n",
    "#Should check if optimus can encode-decode successfully\n",
    "#Adapted from run_latent_generation.py\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import argparse\n",
    "# import glob\n",
    "# import logging\n",
    "import os\n",
    "# import pickle\n",
    "# import random\n",
    "from typing import Tuple, Union, Any, Dict, List\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# import gen_utils as utils # helper functions originally in run_latent_generation.py\n",
    "# from .Args import Args \n",
    "\n",
    "# from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, TensorDataset\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# PARENT = 'Optimus_dir.code'\n",
    "from Optimus_dir.code.pytorch_transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig, BertConfig\n",
    "from Optimus_dir.code.pytorch_transformers import GPT2Tokenizer, GPT2ForLatentConnector #, GPT2LMHeadModel, \n",
    "# from .pytorch_transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer\n",
    "# from .pytorch_transformers import XLNetLMHeadModel, XLNetTokenizer\n",
    "# from .pytorch_transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
    "from Optimus_dir.code.pytorch_transformers import BertForLatentConnector, BertTokenizer\n",
    "\n",
    "from collections import defaultdict\n",
    "from Optimus_dir.code.examples.big_ae.modules import VAE \n",
    "# from .examples.big_ae.utils import (TextDataset_Split, TextDataset_2Tokenizers, BucketingDataLoader)\n",
    "\n",
    "# import pdb\n",
    "\n",
    "class Optimus:\n",
    "    MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "    ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig)), ())\n",
    "\n",
    "    MODEL_CLASSES = {\n",
    "        'gpt2': (GPT2Config, GPT2ForLatentConnector, GPT2Tokenizer),\n",
    "        'bert': (BertConfig, BertForLatentConnector, BertTokenizer)\n",
    "    }\n",
    "\n",
    "    # Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
    "    # in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "    # and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
    "    PADDING_TEXT = \"\"\" In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "    (except for Alexei and Maria) are discovered.\n",
    "    The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "    remainder of the story. 1883 Western Siberia,\n",
    "    a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "    Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "    father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "    man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "    the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "    with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size=32, beta=0.5) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            latent_size (int, optional): _description_. Defaults to 32.\n",
    "            beta (float, optional): _description_. Defaults to 0.5.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: _description_\n",
    "        \"\"\"        \n",
    "        # no model choice for now, fix architecture and checkpoint for simplicity?\n",
    "\n",
    "        directory = 'Optimus_dir/code/checkpoints' # edit if file structure changes\n",
    "        self.latent_size = latent_size\n",
    "        self.num_layers = 12 # correct for GPT-2\n",
    "        self.hidden_size = 768 # correct for GPT-2\n",
    "\n",
    "        train_data_file = f'{directory}/train.txt'\n",
    "        eval_data_file = f'{directory}/test.txt'\n",
    "        if latent_size == 768 and beta == 0.5:\n",
    "            checkpoint_dir = f'{directory}/optimus_latent768_beta05'\n",
    "            self.latent_size = 768\n",
    "        # elif latent_size == 768 and beta == 1.0:\n",
    "        #     checkpoint_dir = f'{directory}/optimus_latent768_beta1'\n",
    "        elif latent_size == 32:\n",
    "            checkpoint_dir = f'{directory}/optimus_latent32_beta05'\n",
    "            self.latent_size = 32\n",
    "        else:\n",
    "            raise ValueError('Only latent size 32 and 768 supported')\n",
    "        # checkpoint_dir = f'{directory}/optimus_latent32_beta05'\n",
    "        \n",
    "        output_dir = f'{directory}/outputs'\n",
    "        # train_data_file = None\n",
    "        # eval_data_file = None\n",
    "        # output_dir = None\n",
    "\n",
    "        args = Args(train_data_file, eval_data_file, checkpoint_dir, output_dir)\n",
    "        # args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        self.device = args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "        args.latent_size = latent_size\n",
    "        # args.beta = beta\n",
    "\n",
    "        Optimus.set_seed(args)\n",
    "\n",
    "        global_step = args.gloabl_step_eval\n",
    "\n",
    "        output_encoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-encoder-{}'.format(global_step))\n",
    "        output_decoder_dir = os.path.join(args.checkpoint_dir, 'checkpoint-decoder-{}'.format(global_step)) \n",
    "        checkpoints = [ [output_encoder_dir, output_decoder_dir] ]\n",
    "\n",
    "        # Load a trained Encoder model and vocabulary that you have fine-tuned\n",
    "        encoder_config_class, encoder_model_class, encoder_tokenizer_class = Optimus.MODEL_CLASSES[args.encoder_model_type]\n",
    "        model_encoder = encoder_model_class.from_pretrained(output_encoder_dir, latent_size=args.latent_size)\n",
    "        tokenizer_encoder = encoder_tokenizer_class.from_pretrained(args.encoder_tokenizer_name if args.encoder_tokenizer_name else args.encoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "        self.tokenizer_encoder = tokenizer_encoder\n",
    "        \n",
    "        model_encoder.to(args.device)\n",
    "        if args.block_size <= 0:\n",
    "            args.block_size = tokenizer_encoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "        args.block_size = min(args.block_size, tokenizer_encoder.max_len_single_sentence)\n",
    "        decoder_config_class, decoder_model_class, decoder_tokenizer_class = Optimus.MODEL_CLASSES[args.decoder_model_type]\n",
    "\n",
    "        model_decoder = decoder_model_class.from_pretrained(\n",
    "            output_decoder_dir, latent_size=args.latent_size,\n",
    "            output_hidden_states=True, # added, hopefully doesn't break rest of code lol\n",
    "            )\n",
    "        self.model_decoder_with_hidden = decoder_model_class.from_pretrained(output_decoder_dir, latent_size=args.latent_size, output_hidden_states=True)\n",
    "        #     tokenizer_decoder = decoder_tokenizer_class.from_pretrained(args.decoder_tokenizer_name if args.decoder_tokenizer_name else args.decoder_model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "        # tokenizer_decoder = AutoTokenizer.from_pretrained(\"gpt2\", use_fast = False) # shitty fix for decoder tokenizer\n",
    "        tokenizer_decoder = GPT2Tokenizer.from_pretrained(\"gpt2\") # shitty fix for decoder tokenizer\n",
    "        self.tokenizer_decoder = tokenizer_decoder\n",
    "        model_decoder.to(args.device)\n",
    "        if args.block_size <= 0:\n",
    "            args.block_size = tokenizer_decoder.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "        args.block_size = min(args.block_size, tokenizer_decoder.max_len_single_sentence)\n",
    "\n",
    "        # Load full model\n",
    "        output_full_dir    = os.path.join(args.checkpoint_dir, 'checkpoint-full-{}'.format(global_step)) \n",
    "        checkpoint = torch.load(os.path.join(output_full_dir, 'training.bin'), map_location=self.device)\n",
    "\n",
    "        # Chunyuan: Add Padding token to GPT2\n",
    "        special_tokens_dict = {'pad_token': '<PAD>', 'bos_token': '<BOS>', 'eos_token': '<EOS>'}\n",
    "        num_added_toks = tokenizer_decoder.add_special_tokens(special_tokens_dict)\n",
    "        print('We have added', num_added_toks, 'tokens to GPT2')\n",
    "        model_decoder.resize_token_embeddings(len(tokenizer_decoder))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "        assert tokenizer_decoder.pad_token == '<PAD>'\n",
    "\n",
    "        self.model_vae = VAE(model_encoder, model_decoder, tokenizer_encoder, tokenizer_decoder, args)\n",
    "        self.model_vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model_vae.to(args.device)\n",
    "        # self.tokenizer_encoder = \n",
    "        self.args = args\n",
    "\n",
    "        bos_id = self.model_vae.tokenizer_decoder.encode('<BOS>')\n",
    "        bos_id = torch.tensor(bos_id, dtype=torch.long, device=self.device)\n",
    "        self.bos_id = bos_id.unsqueeze(0).repeat(1, 1)  \n",
    "\n",
    "    def set_seed(args):\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        if args.n_gpu > 0:\n",
    "            torch.cuda.manual_seed_all(args.seed)\n",
    "    \n",
    "    def latent_code_from_text(self, text: str,) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            text (str): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, int]: _description_\n",
    "        \"\"\"        \n",
    "        tokenized1 = self.model_vae.tokenizer_encoder.encode(text)\n",
    "        tokenized1 = [101] + tokenized1 + [102] # Change this so same tokens in both methods\n",
    "        coded1 = torch.Tensor([tokenized1])\n",
    "        coded1 =torch.Tensor.long(coded1)\n",
    "        with torch.no_grad():\n",
    "            x0 = coded1\n",
    "            x0 = x0.to(self.args.device)\n",
    "            pooled_hidden_fea = self.model_vae.encoder(x0, attention_mask=(x0 > 0).float())[1]\n",
    "            mean, logvar = self.model_vae.encoder.linear(pooled_hidden_fea).chunk(2, -1)\n",
    "            latent_z = mean.squeeze(1)  \n",
    "            coded_length = len(tokenized1)\n",
    "            return latent_z, coded_length\n",
    "\n",
    "    # def logits_from_latent_code(self, latent_z,):\n",
    "    #     # Logits from which position? What did the paper do?\n",
    "    #     raise NotImplementedError\n",
    "\n",
    "    def print_n_texts_from_latent_code(self, latent_z: torch.Tensor, n=5, perturb=0.0) -> None:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            latent_z (torch.Tensor): _description_\n",
    "            n (int, optional): _description_. Defaults to 5.\n",
    "        \"\"\"        \n",
    "        for i in range(n):\n",
    "            text_x1 = self.text_from_latent_code(latent_z, perturb)\n",
    "            print(f'Decoding {i+1}: {text_x1}')\n",
    "\n",
    "    def extract_last_token(hidden_states_tuple: Tuple[torch.Tensor], stack=False) -> Union[torch.Tensor, Tuple[torch.Tensor]]:\n",
    "        \"\"\"also turns on grad\n",
    "\n",
    "        Args:\n",
    "            hidden_states_tuple (Tuple[torch.Tensor]): _description_\n",
    "            stack (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Union[torch.Tensor, Tuple[torch.Tensor]]: _description_\n",
    "        \"\"\"             \n",
    "        # old_shape = hidden_states_tuple[0].shape\n",
    "        # new_shape = (old_shape[0], 1, old_shape[2])\n",
    "        # print(f'old shape: {old_shape}, new shape: {new_shape}')\n",
    "        hidden_states_list = []\n",
    "        for i in range(len(hidden_states_tuple)):\n",
    "            last_token_only = hidden_states_tuple[i][:, -1, :].requires_grad_(True) # for some reason this loses a dimension, also not sure if I need to clone\n",
    "            last_token_only = last_token_only.unsqueeze(1) # may not work if batch size is not 1?\n",
    "            hidden_states_list.append(last_token_only)\n",
    "            # assert last_token_only.shape == new_shape, f'Expected shape {new_shape}, got {last_token_only.shape}'\n",
    "        if stack:\n",
    "            shape = hidden_states_tuple[0].shape\n",
    "\n",
    "            stacked = torch.stack(hidden_states_tuple)\n",
    "            assert stacked[0].shape == shape\n",
    "            return stacked\n",
    "        else:\n",
    "            return tuple(hidden_states_list)\n",
    "\n",
    "    def hidden_from_latent_with_grad(self, latent_z: torch.Tensor,)-> Dict[str, Any]:\n",
    "        \"\"\"Just calls text_from_latent_code with use_grad=True and returns hidden states\n",
    "        Convenience method for backprop\n",
    "        Use greedy sampling for differentiability\n",
    "\n",
    "        Args:\n",
    "            latent_z (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, Tuple[torch.Tensor, ...]]: _description_\n",
    "        \"\"\"        \n",
    "        return self.text_from_latent_code(latent_z, return_dict=True, greedy=True, use_grad=True)\n",
    "\n",
    "    def text_from_latent_code(self, latent_z: torch.Tensor, perturb=0.0, return_hidden=False, greedy=False, use_grad=False, return_dict=False) -> str:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            latent_z (torch.Tensor): shape (1, latent_size)\n",
    "            perturb (float, optional): _description_. Defaults to 0.0.\n",
    "\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"    \n",
    "        if return_dict:\n",
    "            return_hidden = True    \n",
    "\n",
    "        past = latent_z + torch.randn_like(latent_z) * perturb * torch.norm(latent_z)\n",
    "        BOS_token = self.tokenizer_decoder.encode('<BOS>')\n",
    "\n",
    "        length = 150 # maximum length\n",
    "        out = Optimus.sample_sequence_conditional(\n",
    "            model=self.model_vae.decoder,\n",
    "            context=BOS_token,\n",
    "            past=past,\n",
    "            length= length, # Chunyuan: Fix length; or use <EOS> to complete a sentence\n",
    "            temperature=self.args.temperature,\n",
    "            top_k=self.args.top_k,\n",
    "            top_p=self.args.top_p,\n",
    "            device=self.args.device,\n",
    "            decoder_tokenizer = self.tokenizer_decoder,\n",
    "            return_hidden = return_hidden,\n",
    "            greedy=greedy,\n",
    "            use_grad=use_grad,\n",
    "        )\n",
    "        if return_hidden:\n",
    "            out, hidden = out\n",
    "            for tensor in hidden:\n",
    "                tensor.requires_grad_(True)\n",
    "        tokens = out[0,:].tolist()\n",
    "        text_x1 = self.tokenizer_decoder.decode(tokens, clean_up_tokenization_spaces=True)\n",
    "        text_x1 = text_x1.split()[1:-1]\n",
    "        text_x1 = ' '.join(text_x1)\n",
    "        \n",
    "        if return_dict:\n",
    "            res = {'hidden': hidden, 'text': text_x1, 'num_tokens': len(tokens)}\n",
    "            return res\n",
    "        elif return_hidden:\n",
    "            return text_x1, hidden, len(tokens)\n",
    "        else:\n",
    "            return text_x1\n",
    "    \n",
    "    def get_token_ids(self, text: str) -> List[int]:\n",
    "        \"\"\"also prepends BOS token\n",
    "\n",
    "        Args:\n",
    "            text (str): _description_\n",
    "\n",
    "        Returns:\n",
    "            List[int]: _description_\n",
    "        \"\"\"        \n",
    "        token_ids = self.model_vae.tokenizer_decoder.encode('<BOS>' + text)\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long, device=self.device)\n",
    "        return token_ids.unsqueeze(0)\n",
    "\n",
    "    def text_to_latent_to_text_activations(self, text: str, averaging_num=1, greedy=True) -> torch.Tensor:\n",
    "        \"\"\"given text, encode into latent, then generate averaging_num times with latent and BOS, and return averaged activations\n",
    "\n",
    "        Args:\n",
    "            text (str): _description_\n",
    "            averaging_num (int, optional): number of times to generate text and average hidden state. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: last token activations of shape (self.num_layers + 1, 1, 1, self.latent_size)\n",
    "        \"\"\"       \n",
    "        latent_z = self.latent_code_from_text(text)[0]\n",
    "        # inputs = {'input_ids': self.bos_id, 'past': latent_z}\n",
    "        # if greedy:\n",
    "        #     raise NotImplementedError\n",
    "\n",
    "        #2. Put latent through vae decoder and Get hidden state \n",
    "        for i in range(averaging_num):\n",
    "            out = self.hidden_from_latent_with_grad(latent_z)\n",
    "            text, hidden_states_unformatted = out['text'], out['hidden']\n",
    "            hidden_states_last_token = Optimus.extract_last_token(hidden_states_unformatted)\n",
    "            hidden_states_last_token = torch.stack(hidden_states_last_token)\n",
    "            # hidden_states_all_tokens = torch.stack(hidden_states_unformatted) # hidden_states reformated from list of tensors to single tensor\n",
    "            # hidden_states_last_token = hidden_states_all_tokens[..., -1, :]\n",
    "            if i == 0:\n",
    "                hidden_states_last_token_averaged = hidden_states_last_token.clone().to(self.device)\n",
    "            else:\n",
    "                hidden_states_last_token_averaged += hidden_states_last_token\n",
    "        hidden_states_last_token_averaged /= averaging_num\n",
    "        if hidden_states_last_token_averaged.shape != (self.num_layers + 1, 1, 1, self.hidden_size):\n",
    "            # print(hidden_states_last_token_averaged.shape)\n",
    "            raise ValueError('hidden_states_last_token_averaged has wrong shape')\n",
    "        return text, hidden_states_last_token_averaged\n",
    "        \n",
    "        # hidden_states = self.model_decoder_with_hidden(**inputs)[2] # copied from Optimus.sample_sequence_conditional, don't know why this works\n",
    "        # current_direction = torch.stack(hidden_states).view(-1) # this may stack from multiple sequence inputs, check\n",
    "\n",
    "\n",
    "    def sample_sequence_conditional(\n",
    "            model, length, context, past=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, device='cpu', decoder_tokenizer=None, return_hidden=False,\n",
    "            greedy=False, use_grad=False\n",
    "            ):\n",
    "        context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "        context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "        generated = context\n",
    "        eos = False\n",
    "        if not use_grad:\n",
    "            context = torch.no_grad()\n",
    "        else:\n",
    "            context = torch.enable_grad()  # This is a no-op context manager\n",
    "\n",
    "        with context:\n",
    "            # while True:\n",
    "            for _ in range(length):\n",
    "                inputs = {'input_ids': generated, 'past': past}\n",
    "                outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "                next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                if greedy:\n",
    "                    # assert next_token_logits.shape == (50257,)\n",
    "                    next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "                    # generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "                else:\n",
    "                    filtered_logits = Optimus.top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "                    next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "                # pdb.set_trace()\n",
    "                if next_token.unsqueeze(0)[0,0].item() == decoder_tokenizer.encode('<EOS>')[0]:\n",
    "                    eos = True\n",
    "                    break\n",
    "        if not eos:\n",
    "            print('Reached maximum length without <EOS> token')\n",
    "        if return_hidden:\n",
    "            return generated, outputs[2] # return hidden state as of generating last token\n",
    "        else:\n",
    "            return generated\n",
    "    \n",
    "    def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "        \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "            Args:\n",
    "                logits: logits distribution shape (vocabulary size)\n",
    "                top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "                top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                    Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "            From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "        \"\"\"\n",
    "        assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "        top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "        if top_k > 0:\n",
    "            # Remove all tokens with a probability less than the last token of the top-k\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "\n",
    "        if top_p > 0.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the indices to the right to keep also the first token above the threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        return logits\n",
    "    \n",
    "    def is_latent(self, x):\n",
    "        return isinstance(x, torch.Tensor) and x.shape == (1, self.args.latent_size)\n",
    "    \n",
    "    def print_text(self, *args, n=5, perturb=0.0):\n",
    "        for arg in args:\n",
    "            if isinstance(arg, str):\n",
    "                print(f'\"{arg}\" encoded and decoded:')\n",
    "                latent = self.latent_code_from_text(arg)[0]\n",
    "            elif self.is_latent(arg):\n",
    "                latent = arg\n",
    "            else:\n",
    "                raise ValueError('Input must be string or latent')\n",
    "            self.print_n_texts_from_latent_code(latent, n, perturb)\n",
    "            print('\\n')\n",
    "    \n",
    "    def interpolate(self, num_interpolation_steps):   \n",
    "        latent_z1, coded_length1 = self.latent_code_from_text()\n",
    "        latent_z2, coded_length2 = self.latent_code_from_text(self.args.sent_target, self.tokenizer_encoder, self.model_vae, self.args)\n",
    "\n",
    "        result = defaultdict(str)\n",
    "\n",
    "        num_steps = num_interpolation_steps + 1\n",
    "        for step in range(num_steps+1):\n",
    "            latent_z = latent_z1 + (latent_z2 - latent_z1) * step * 1.0/num_steps\n",
    "            \n",
    "            text_interpolate = self.text_from_latent_code(latent_z, self.model_vae, self.args, self.tokenizer_decoder)\n",
    "            result[step] = text_interpolate\n",
    "            print(text_interpolate)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def print_greedy(self, latent):\n",
    "        text = self.text_from_latent_code(latent, greedy=True)\n",
    "        print(text)\n",
    "\n",
    "\n",
    "class Args:\n",
    "    #     parser = argparse.ArgumentParser()\n",
    "    def __init__(self, train_data_file, eval_data_file, checkpoint_dir, output_dir, checkpoint_number=508523):\n",
    "        #     parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n",
    "        #                         help=\"The input training data file (a text file).\")\n",
    "        self.train_data_file = train_data_file\n",
    "        #     parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
    "        #                         help=\"An input evaluation data file to evaluate the perplexity on (a text file).\")\n",
    "        self.eval_data_file = eval_data_file\n",
    "        #     parser.add_argument(\"--checkpoint_dir\", default=None, type=str, required=True,\n",
    "        #                         help=\"The directory where checkpoints are saved.\")\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        #     parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "        #                         help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "        self.output_dir = output_dir\n",
    "        #     parser.add_argument(\"--dataset\", default='Snli', type=str, help=\"The dataset.\")\n",
    "        self.dataset = 'Snli' # may have to change to wiki?\n",
    "\n",
    "        #     ## Variational auto-encoder\n",
    "        #     parser.add_argument(\"--latent_size\", default=32, type=int, help=\"Latent space dimension.\")\n",
    "        self.latent_size = 768\n",
    "        #     parser.add_argument(\"--total_sents\", default=10, type=int, help=\"Total sentences to test recontruction.\")\n",
    "        self.total_sents = 10\n",
    "        #     parser.add_argument(\"--num_interpolation_steps\", default=10, type=int, help=\"Total sentences to test recontruction.\")\n",
    "        self.num_interpolation_steps = 10\n",
    "        #     parser.add_argument(\"--play_mode\", default=\"interpolation\", type=str,\n",
    "        #                         help=\"interpolation or reconstruction.\")\n",
    "        self.play_mode = \"interpolation\"\n",
    "\n",
    "\n",
    "        #     ## Encoder options\n",
    "        #     parser.add_argument(\"--encoder_model_type\", default=\"bert\", type=str,\n",
    "        #                         help=\"The encoder model architecture to be fine-tuned.\")\n",
    "        self.encoder_model_type = \"bert\"\n",
    "        #     parser.add_argument(\"--encoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "        #                         help=\"The encoder model checkpoint for weights initialization.\")\n",
    "        self.encoder_model_name_or_path = \"bert-base-cased\"\n",
    "        #     parser.add_argument(\"--encoder_config_name\", default=\"\", type=str,\n",
    "        #                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "        self.encoder_config_name = \"\"\n",
    "        #     parser.add_argument(\"--encoder_tokenizer_name\", default=\"\", type=str,\n",
    "        #                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "        self.encoder_tokenizer_name = \"\"\n",
    "\n",
    "            ## Decoder options\n",
    "        #     parser.add_argument(\"--decoder_model_type\", default=\"gpt2\", type=str,\n",
    "        #                         help=\"The decoder model architecture to be fine-tuned.\")\n",
    "        self.decoder_model_type = \"gpt2\"\n",
    "        #     parser.add_argument(\"--decoder_model_name_or_path\", default=\"bert-base-cased\", type=str,\n",
    "        #                         help=\"The decoder model checkpoint for weights initialization.\")\n",
    "        self.decoder_model_name_or_path = \"bert-base-cased\"\n",
    "        #     parser.add_argument(\"--decoder_config_name\", default=\"\", type=str,\n",
    "        #                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "        self.decoder_config_name = \"\"\n",
    "        #     parser.add_argument(\"--decoder_tokenizer_name\", default=\"\", type=str,\n",
    "        #                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "        self.decoder_tokenizer_name = \"\"\n",
    "\n",
    "        #     parser.add_argument(\"--per_gpu_train_batch_size\", default=1, type=int,\n",
    "        #                         help=\"Batch size per GPU/CPU for training.\")\n",
    "        self.per_gpu_train_batch_size = 1\n",
    "        #     parser.add_argument(\"--per_gpu_eval_batch_size\", default=1, type=int,\n",
    "        #                         help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "        self.per_gpu_eval_batch_size = 1\n",
    "        #     parser.add_argument('--gloabl_step_eval', type=int, default=661,\n",
    "        #                         help=\"Evaluate the results at the given global step\")\n",
    "        self.gloabl_step_eval = checkpoint_number\n",
    "        #     parser.add_argument(\"--max_seq_length\", default=512, type=int,\n",
    "        #                         help=\"Optional input sequence length before tokenization. The sequence will be dropped if it is longer the max_seq_length\")\n",
    "        self.max_seq_length = 512\n",
    "        #     # Interact with users\n",
    "        #     parser.add_argument(\"--interact_with_user_input\", action='store_true', help=\"Use user input to interact_with.\")\n",
    "        self.interact_with_user_input = False\n",
    "        #     parser.add_argument(\"--sent_source\", type=str, default=\"\")\n",
    "        self.sent_source = \"\"\n",
    "        #     parser.add_argument(\"--sent_target\", type=str, default=\"\")\n",
    "        self.sent_target = \"\"\n",
    "        #     parser.add_argument(\"--sent_input\", type=str, default=\"\")\n",
    "        self.sent_input = \"\"\n",
    "        #     parser.add_argument(\"--degree_to_target\", type=float, default=\"1.0\")\n",
    "        self.degree_to_target = 1.0\n",
    "\n",
    "        #     ## Variational auto-encoder\n",
    "        #     parser.add_argument(\"--nz\", default=32, type=int,\n",
    "        #                         help=\"Latent space dimension.\")\n",
    "        self.nz = 32\n",
    "\n",
    "        #     parser.add_argument(\"--prompt\", type=str, default=\"\")\n",
    "        self.prompt = \"\"\n",
    "        #     parser.add_argument(\"--padding_text\", type=str, default=\"\")\n",
    "        self.padding_text = \"\"\n",
    "        #     parser.add_argument(\"--length\", type=int, default=20)\n",
    "        self.length = 20\n",
    "        #     parser.add_argument(\"--temperature\", type=float, default=1.0)\n",
    "        self.temperature = 1.0\n",
    "        #     parser.add_argument(\"--top_k\", type=int, default=0)\n",
    "        self.top_k = 0\n",
    "        #     parser.add_argument(\"--top_p\", type=float, default=1.0)\n",
    "        self.top_p = 1.0\n",
    "        #     parser.add_argument(\"--no_cuda\", action='store_true',\n",
    "        #                         help=\"Avoid using CUDA when available\")\n",
    "        self.no_cuda = False\n",
    "        #     parser.add_argument('--seed', type=int, default=42,\n",
    "        #                         help=\"random seed for initialization\")\n",
    "        self.seed = 42\n",
    "\n",
    "        #     parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "        #                         help=\"Optional input sequence length after tokenization.\"\n",
    "        #                              \"The training dataset will be truncated in block of this size for training.\"\n",
    "        #                              \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "        self.block_size = -1\n",
    "        #     parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "        #                         help=\"Set this flag if you are using an uncased model.\")\n",
    "        self.do_lower_case = False\n",
    "\n",
    "        #     parser.add_argument(\"--use_philly\", action='store_true',\n",
    "        #                         help=\"Use Philly for computing.\")\n",
    "        self.use_philly = False\n",
    "\n",
    "        #     args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Optimus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 3 tokens to GPT2\n"
     ]
    }
   ],
   "source": [
    "#TODO decouple from Optimus\n",
    "vae32 = Optimus(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Optimus with latent_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Optimus with latent_size 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae768 = Optimus(768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare effect of latent size\n",
    "\n",
    "Seems like latent_size=32 makes the sentences \"closer\"? But reconstruction seems worse\n",
    "\n",
    "Observations:\n",
    "- Some latents get \"translated\" into either nothing or seemingly meaningless rambling. Some latents produce semantically similar words though.\n",
    "- Longer sentences seem to reconstruct better than words/short sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden = vae32.get_activations(vae32.latent_code_from_text('hello')[0], 'hello')\n",
    "# print (hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae32.print_text(\n",
    "#     \"love\", \n",
    "#     'hate', \n",
    "#     'I love you so much!', \n",
    "#     'I hate you so much!'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae768.print_text(\n",
    "#     \"love\", \n",
    "#     'hate', \n",
    "#     'I love you so much!', \n",
    "#     'I hate you so much!'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae32.print_text(shaq, benzene)\n",
    "# baseline_sports = \"basketball \" * 20\n",
    "# baseline_chemistry = \"chemistry \" * 20\n",
    "# vae32.print_text(baseline_sports, baseline_chemistry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae768.print_text(shaq, benzene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae32.print_text(shaq, benzene, perturb=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae768.print_text(shaq, benzene, perturb=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimus\n",
    "# text_love = \"I love you so much!\"\n",
    "# latent_love = vae32.latent_code_from_text(text_love)[0]\n",
    "# # text_hate = \"I hate you so much!\"\n",
    "# # latent_hate = vae32.latent_code_from_text(text_hate)[0]\n",
    "# print(latent_love.shape)\n",
    "# # print(len(text_original))\n",
    "# vae32.print_n_texts_from_latent_code(latent_love, 10)\n",
    "# vae32.print_n_texts_from_latent_code(latent_hate, 10)\n",
    "# vae32.print_n_texts_from_latent_code(latent_love - latent_hate, 10)\n",
    "# vae32.print_n_texts_from_latent_code(-latent_love + latent_hate, 10)\n",
    "# for i in range(10):\n",
    "#     print(vae.text_from_latent_code(latent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Visualization Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "def cosim(a, b):\n",
    "    return nn.functional.cosine_similarity(a, b, dim=0)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self, optimus: Optimus, target_dir, training_args, latent_module, logging_steps=1e2,\n",
    "        length_reg=0.0,\n",
    "        ):\n",
    "        \"\"\"_summary_\n",
    "        Example use case: target_dir is hidden of last token of shaq\n",
    "        loss is 1 - cosine_similarity(hidden - target)\n",
    "        Loss transposed to be min at 0, max at 2\n",
    "\n",
    "        Args:\n",
    "            optimus (Optimus): _description_\n",
    "            target_dir (_type_): _description_\n",
    "            training_args (_type_): _description_\n",
    "            latent (_type_): _description_\n",
    "        \"\"\"        \n",
    "        '''\n",
    "        decoder: decoder model from an Optimus VAE, must have output_hidden_states=True\n",
    "        target_dir: of size (1 + num_layers, 1, 1, latent_size)\n",
    "        '''\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        latent_module.to(self.device)\n",
    "        super().__init__(model=latent_module, args=training_args)\n",
    "        self.optimus = optimus # make sure this outputs hidden\n",
    "        self.target_dir = target_dir.to(self.device).view(-1) #flattened\n",
    "        # assert target_dir.shape == \n",
    "        context = optimus.model_vae.tokenizer_decoder.encode('<BOS>')\n",
    "        context = torch.tensor(context, dtype=torch.long, device=self.device)\n",
    "        self.context = context.unsqueeze(0).repeat(1, 1)  \n",
    "        self.loss_values = []\n",
    "        self.logging_steps = logging_steps\n",
    "        self.length_reg = length_reg\n",
    "\n",
    "    def compute_loss(self, latent_module, return_cosim=False,\n",
    "        # return_dir=False,\n",
    "        ): \n",
    "        '''\n",
    "        latent is a trainable parameter/model with shape [1, latent_size]\n",
    "        '''\n",
    "        #1. Extract params from latent model\n",
    "        latent = latent_module.get_parameter('latent').clone().requires_grad_(True)\n",
    "        # inputs = {'input_ids': self.context, 'past': past}\n",
    "        \n",
    "        #2. Put latent through vae decoder and Get hidden state\n",
    "        # hidden_states = self.decoder(**inputs)[2] \n",
    "        out = self.optimus.hidden_from_latent_with_grad(latent,) #generates until EOS and returns hidden state\n",
    "        text, hidden_states_tuple, output_length = out['text'], out['hidden'], out['num_tokens']\n",
    "        \n",
    "        hidden_states_last_token = Optimus.extract_last_token(hidden_states_tuple)\n",
    "        hidden_states_last_token = torch.stack(hidden_states_last_token) # hidden_states reformated from tuple of tensors to single tensor\n",
    "        # print(f'hidden_states_last_token shape: {hidden_states_last_token.shape}')\n",
    "        # hidden_states_last_token = hidden_states_all_tokens[..., -1, :]\n",
    "        current_dir = hidden_states_last_token.view(-1) #flatten hidden layers into 1d vector\n",
    "        if current_dir.shape != self.target_dir.shape:\n",
    "            print(f'Target shape is {self.target_dir.shape} but current_dir shape is {current_dir.shape}')\n",
    "            raise ValueError\n",
    "        #3. Compute loss with cosine similarity between hidden states\n",
    "        similarity = cosim(current_dir, self.target_dir) \n",
    "        loss = 1 - similarity + self.length_reg * output_length ** 2\n",
    "        assert loss.numel() == 1, \"Loss must be a scalar\"\n",
    "        # assert loss <= 2 and loss >= 0\n",
    "        \n",
    "        return (loss, similarity) if return_cosim else loss\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        num_epochs = int(self.args.num_train_epochs)\n",
    "        latent_module = self.model\n",
    "        for epoch in range(num_epochs):\n",
    "            # for step, batch in enumerate(self.get_train_dataloader()):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = self.compute_loss(latent_module)\n",
    "            loss.backward(\n",
    "                retain_graph=True\n",
    "                )\n",
    "            \n",
    "            latent = latent_module.get_parameter('latent')\n",
    "            if torch.all(latent.grad == 0):\n",
    "                print('Latent grad is 0')\n",
    "\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(f'Before step: {name}, {param.data}')\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # After optimizer.step()\n",
    "            # for name, param in self.model.named_parameters():\n",
    "            #     print(f'After step: {name}, {param.data}')\n",
    "            loss_scalar = loss.item()\n",
    "            self.loss_values.append(loss_scalar)\n",
    "            if epoch % self.logging_steps == 0:\n",
    "                print(f\"Epoch {epoch}\")\n",
    "                self.optimus.print_greedy(latent)\n",
    "                print(f'Loss = 1 - cosine_similarity = {loss_scalar}')\n",
    "                # print(text)\n",
    "            # print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        plt.plot(self.loss_values)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss = 1 - Cosine_Similarity(hidden_state, target_direction)')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer and run loop\n",
    "\n",
    "# dummy_data = {'dummy':'dummy',} # may be needed to appease Trainer\n",
    "# dummy_data = Dataset.from_dict(dummy_data)\n",
    "\n",
    "NUM_LAYERS = 12\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 1\n",
    "HIDDEN_SIZE = 768\n",
    "\n",
    "def train(\n",
    "        optimus: Optimus, target_dir=None, lr=1e-4, num_epochs=1e3, logging_steps=100, init_norm=1.0, init_latent=None, \n",
    "        length_reg = 0.0 #1e-2\n",
    "        ):\n",
    "    latent_size = optimus.latent_size\n",
    "    training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "    training_args.num_train_epochs = num_epochs\n",
    "    training_args.learning_rate = lr\n",
    "\n",
    "    target_dir_shape = (NUM_LAYERS + 1, BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE)\n",
    "    # decoder = vae.model_decoder_with_hidden.to(device)\n",
    "    if target_dir is None:\n",
    "        target_dir = torch.randn(target_dir_shape).to(device) \n",
    "    else:\n",
    "        assert target_dir.shape == target_dir_shape, \"target_dir must have shape (NUM_LAYERS + 1, BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE)\"\n",
    "    latent = nn.Module()\n",
    "    if init_latent == None:\n",
    "        param = torch.randn(1, latent_size)\n",
    "        param /= param.norm() * init_norm\n",
    "    else:\n",
    "        assert init_latent.shape == (1, latent_size)\n",
    "        param = init_latent\n",
    "    param = nn.Parameter(data=param, requires_grad=True)\n",
    "    param_init = param.clone().detach().to(device)\n",
    "    latent.register_parameter(\"latent\", param)\n",
    "    latent.to(device)\n",
    "    # original_latent = param.clone().detach().to(device)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        optimus, target_dir, training_args, latent, logging_steps=logging_steps,\n",
    "        length_reg=length_reg,\n",
    "        # train_dataset=dummy_data,\n",
    "        )\n",
    "    trainer.train()\n",
    "    # optimus.print_greedy(latent.get_parameter('latent'))\n",
    "    # latent_diff = (param - param_init).view(-1).norm()\n",
    "    # print(latent_diff)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations = vae32.text_from_latent_code(torch.randn(1, 32).to(device), return_hidden=True)[1]\n",
    "# stacked = torch.stack(activations)\n",
    "# print(stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D'Angelo is a former professional boxer, who has won five world titles, including the WBA Light Middleweight Title, the WBA Light Middleweight Title, the WBA Light Middleweight Title, the WBA Light Middleweight Title, and the WBA Light Middleweight Title.\n",
      "2.6317951679229736\n"
     ]
    }
   ],
   "source": [
    "# shaq_activations = vae32.text_to_latent_to_activations(shaq, averaging_num=1)\n",
    "#TODO maybe try training with regularization against length of output\n",
    "shaq_latent_decoded, shaq_activations = vae32.text_to_latent_to_text_activations(shaq, greedy=True)\n",
    "print(shaq_latent_decoded)\n",
    "shaq_latent_norm = vae32.latent_code_from_text(shaq,)[0].norm().item()\n",
    "print(shaq_latent_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a synthetic compound of the ethyl alcohols, ethyl alcohols, and ethyl alcohols, which are used in the manufacture of alcohols, and is used in the manufacture of alcohols, and in the manufacture of alcohols, and in the manufacture of alcohols.\n",
      "2.8745837211608887\n"
     ]
    }
   ],
   "source": [
    "benzene_latent_decoded, benzene_activations = vae32.text_to_latent_to_text_activations(benzene, greedy=True)\n",
    "print(benzene_latent_decoded)\n",
    "benzene_latent = vae32.latent_code_from_text(benzene,)[0]\n",
    "benzene_latent_norm = benzene_latent.norm().item()\n",
    "print(benzene_latent_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "The first of these was the \"Bergamasque\" (Bergamasque) program, which was created in the late 1980s and early 1990s.\n",
      "Loss = 1 - cosine_similarity = 0.33134615421295166\n",
      "Epoch 50\n",
      "The first of these was the \"Bergamasque\" (Bergamasque) in the 1990s, which was the first to be certified by the National Academy of Sciences.\n",
      "Loss = 1 - cosine_similarity = 0.06741493940353394\n",
      "Epoch 100\n",
      "The first of the three to be inducted into the Hall of Fame was the former \"D.C. United\" player, John \"D.C. United\" Johnson.\n",
      "Loss = 1 - cosine_similarity = 0.06488639116287231\n",
      "Epoch 150\n",
      "The first female director of the school was Mary Ann L. Smith, who was born in the United States in 1963.\n",
      "Loss = 1 - cosine_similarity = 0.06043362617492676\n",
      "Epoch 200\n",
      "The first female director of the school was Mary Ann L. Smith, who was born in the United States in 1963.\n",
      "Loss = 1 - cosine_similarity = 0.055973589420318604\n",
      "Epoch 250\n",
      "He is the only member of the \"Pioneer\" to have played in the National Football League, and has been named the \"Pioneer of the Year\" by the NFL.\n",
      "Loss = 1 - cosine_similarity = 0.0473790168762207\n",
      "Epoch 300\n",
      "He is the only member of the band to have played in the United States, and has been inducted into the Rock and Roll Hall of Fame in 2006.\n",
      "Loss = 1 - cosine_similarity = 0.04002809524536133\n",
      "Epoch 350\n",
      "He is the only player to have played in the National Hockey League (NHL) since the 1980s, and has played in the NHL since the 1990s.\n",
      "Loss = 1 - cosine_similarity = 0.03234899044036865\n",
      "Epoch 400\n",
      "He is the only player to have played in the National Hockey League (NHL) since the 1980s, and has played in the NHL since the 1990s.\n",
      "Loss = 1 - cosine_similarity = 0.030653417110443115\n",
      "Epoch 450\n",
      "He is the only player to have played in the National Hockey League (NHL) since the 1980s, and has played in the NHL since the 1990s.\n",
      "Loss = 1 - cosine_similarity = 0.029287099838256836\n",
      "Epoch 500\n",
      "He is the only player to have played in the top ten in the Canadian Football League (CFL) since the 1980s, and has been named the Canadian Football League's Most Valuable Player.\n",
      "Loss = 1 - cosine_similarity = 0.026417255401611328\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#TODO: maybe fix retain_graph if speed is a bottleneck\n",
    "train(vae32, target_dir=shaq_activations, lr=1e-3, num_epochs=501, logging_steps=50, \n",
    "      init_norm=shaq_latent_norm,\n",
    "      # init_latent=benzene_latent,\n",
    "      )\n",
    "# train(vae768, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like post training we go to one/zero word sentences (from a random init latent). Maybe will help if we init from an actual sentence\n",
    "\n",
    "Also may be a signal that the latent space is less 'dense' with meaningful sentences than we'd like. I thought using the latent=32 vae would help as sentences will be more 'densely packed' but empirically maybe this is evidence against that.\n",
    "\n",
    "May also need to regularize to penalize distance against actual meaningful sentences in the latent space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe72f77ffa848359958ad0f15b3ec23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'push_to_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/data/joshua_clymer/spar-red-team/owen/visualization_demo.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcompute.safe.ai/data/joshua_clymer/spar-red-team/owen/visualization_demo.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vae32\u001b[39m.\u001b[39;49mmodel_vae\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39m\u001b[39mOptimus_VAE\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/redteam2/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'push_to_hub'"
     ]
    }
   ],
   "source": [
    "vae32.model_vae.push_to_hub(\"Optimus_VAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-redteam2",
   "language": "python",
   "name": "redteam2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
